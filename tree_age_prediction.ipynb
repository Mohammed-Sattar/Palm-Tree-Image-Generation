{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 11:08:44.170660: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 11:08:44.229311: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 11:08:44.281003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731917324.332391    2833 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731917324.346235    2833 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 11:08:44.455179: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1731920230.685488    2833 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 38 Testing Data: 10\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "tree_data_path = \"./archive/tree.csv\"\n",
    "tree_data = pd.read_csv(tree_data_path)\n",
    "\n",
    "# Select relevant columns\n",
    "tree_data = tree_data[['Tree ID', 'Tree Type', 'Age (Estimated)']]\n",
    "\n",
    "# Encode Tree ID and Tree Type into numerical values\n",
    "encoder_id = LabelEncoder()\n",
    "tree_data['Tree ID Encoded'] = encoder_id.fit_transform(tree_data['Tree ID'])\n",
    "\n",
    "# Define X and y\n",
    "X = tree_data['Tree ID'].values  # Input features (Tree IDs)\n",
    "y = tree_data['Age (Estimated)'].values  # Output labels (Ages)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the split\n",
    "print(\"Training Data:\", len(X_train), \"Testing Data:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Tree ID Tree Type  Age (Estimated)  Tree ID Encoded\n",
      "0   AT3001     Amber               10                0\n",
      "1   HT3001     Helwa               25                6\n",
      "2   HT3002     Helwa               25                7\n",
      "3   HT3003     Helwa               25                8\n",
      "4   HT3004     Helwa               25                9\n",
      "5   JT3001      Ajwa               25               31\n",
      "6   KT3001   Sukkari               25               40\n",
      "7   QT3001      Saqi               25               41\n",
      "8   WT3001    Safawi               25               42\n",
      "9   WT3002    Safawi               25               43\n",
      "10  WT3003    Safawi               25               44\n",
      "11  WT3004    Safawi               25               45\n",
      "12  JT2001      Ajwa               17               10\n",
      "13  JT2002      Ajwa               17               11\n",
      "14  JT2003      Ajwa               17               12\n",
      "15  JT2004      Ajwa               17               13\n",
      "16  JT2005      Ajwa               17               14\n",
      "17  JT2006      Ajwa               17               15\n",
      "18  JT2007      Ajwa               17               16\n",
      "19  JT2008      Ajwa               17               17\n",
      "20  JT2009      Ajwa               17               18\n",
      "21  JT2010      Ajwa               17               19\n",
      "22  JT2011      Ajwa               17               20\n",
      "23  JT2012      Ajwa               17               21\n",
      "24  JT2013      Ajwa               17               22\n",
      "25  JT2014      Ajwa               17               23\n",
      "26  JT2015      Ajwa               17               24\n",
      "27  JT2016      Ajwa               17               25\n",
      "28  JT2017      Ajwa               17               26\n",
      "29  JT2018      Ajwa               17               27\n",
      "30  JT2019      Ajwa                8               28\n",
      "31  JT2020      Ajwa                8               29\n",
      "32  JT2021      Ajwa               17               30\n",
      "33  BT4001     Barhi               37                1\n",
      "34  BT4002     Barhi               37                2\n",
      "35  BT4003     Barhi               37                3\n",
      "36  BT4004     Barhi               37                4\n",
      "37  BT5001     Barhi               25                5\n",
      "38  WT5001    Safawi               40               46\n",
      "39  WT5002    Safawi               40               47\n",
      "40  JT5001      Ajwa               25               32\n",
      "41  JT5002      Ajwa               22               33\n",
      "42  JT5003      Ajwa               25               34\n",
      "43  JT5004      Ajwa               25               35\n",
      "44  JT5005      Ajwa                3               36\n",
      "45  JT5006      Ajwa               25               37\n",
      "46  JT5007      Ajwa               27               38\n",
      "47  JT5008      Ajwa                3               39\n"
     ]
    }
   ],
   "source": [
    "print(tree_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Type\n",
      "Ajwa       30\n",
      "Safawi      6\n",
      "Barhi       5\n",
      "Helwa       4\n",
      "Amber       1\n",
      "Saqi        1\n",
      "Sukkari     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tree_type_counts = tree_data['Tree Type'].value_counts()\n",
    "print(tree_type_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age (Estimated)\n",
      "17    19\n",
      "25    16\n",
      "37     4\n",
      "40     2\n",
      "8      2\n",
      "3      2\n",
      "10     1\n",
      "22     1\n",
      "27     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tree_age_counts = tree_data['Age (Estimated)'].value_counts()\n",
    "print(tree_age_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load all images for a given tree ID\n",
    "def load_images_for_tree(tree_id):\n",
    "    images = []\n",
    "    for file_name in os.listdir(IMAGE_DIR):\n",
    "        if file_name.startswith(tree_id) and file_name.endswith(\".jpg\"):\n",
    "            # Load image, resize, and convert to array\n",
    "            img_path = os.path.join(IMAGE_DIR, file_name)\n",
    "            img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "            img_array = img_to_array(img) / 255.0  # Normalize to [0, 1]\n",
    "            images.append(img_array)\n",
    "    return np.stack(images) if images else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset of image tensors and labels\n",
    "def build_image_dataset(tree_ids, ages):\n",
    "    image_data = []\n",
    "    age_labels = []\n",
    "    for tree_id, age in zip(tree_ids, ages):\n",
    "        images = load_images_for_tree(tree_id)\n",
    "        if images is not None:\n",
    "            image_data.append(images)\n",
    "            age_labels.append(age)\n",
    "    return np.array(image_data, dtype=object), np.array(age_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMG_HEIGHT, IMG_WIDTH = 512, 512  # Resize dimensions\n",
    "IMAGE_DIR = \"./archive/Palm Trees/Palm Trees/\"  # Directory containing images\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "X_train_images, y_train_labels = build_image_dataset(X_train, y_train)\n",
    "X_test_images, y_test_labels = build_image_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: X=, 34, y=34\n",
      "Testing data shape:, X=10, y=10\n"
     ]
    }
   ],
   "source": [
    "# Print dataset shapes\n",
    "print(f\"Training data shape: X=, {len(X_train_images)}, y={len(y_train_labels)}\")\n",
    "print(f\"Testing data shape:, X={len(X_test_images)}, y={len(y_test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 34\n",
      "\n",
      "Sample 1:\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (16, 512, 512, 3)\n",
      "Pixel Value Range: Min = 0.0 , Max = 1.0\n",
      "\n",
      "Sample 2:\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (12, 512, 512, 3)\n",
      "Pixel Value Range: Min = 0.0 , Max = 1.0\n",
      "\n",
      "Sample 3:\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (16, 512, 512, 3)\n",
      "Pixel Value Range: Min = 0.0 , Max = 1.0\n",
      "\n",
      "Sample 4:\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (12, 512, 512, 3)\n",
      "Pixel Value Range: Min = 0.0 , Max = 1.0\n",
      "\n",
      "Sample 5:\n",
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (12, 512, 512, 3)\n",
      "Pixel Value Range: Min = 0.0 , Max = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check the number of samples in the dataset\n",
    "print(\"Number of training samples:\", len(X_train_images))\n",
    "\n",
    "# Iterate over a few samples to inspect their details\n",
    "for idx, sample in enumerate(X_train_images[:5]):  # Inspect first 5 samples\n",
    "    print(f\"\\nSample {idx + 1}:\")\n",
    "    \n",
    "    # Check the type\n",
    "    print(\"Type:\", type(sample))  # Should be a numpy.ndarray\n",
    "    \n",
    "    # Check the shape\n",
    "    print(\"Shape:\", sample.shape)  # Should be (n_images, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "    \n",
    "    # Check pixel value range\n",
    "    print(\"Pixel Value Range: Min =\", sample.min(), \", Max =\", sample.max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohammed/anaconda3/envs/AI305_ML/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "W0000 00:00:1731888380.270706    2984 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">126,272</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m512\u001b[0m, │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m3\u001b[0m)                     │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m126,272\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,529</span> (494.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m126,529\u001b[0m (494.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,529</span> (494.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m126,529\u001b[0m (494.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameters\n",
    "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 512, 512, 3  # Image dimensions\n",
    "FEATURE_DIM = 256  # Feature vector size\n",
    "\n",
    "# Define the CNN model for feature extraction\n",
    "def build_feature_extractor():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(FEATURE_DIM, activation='relu')  # Feature vector for each image\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build the model for multiple images per tree\n",
    "def build_tree_age_predictor():\n",
    "    # Feature extractor\n",
    "    feature_extractor = build_feature_extractor()\n",
    "    \n",
    "    # Input for multiple images of a tree\n",
    "    image_inputs = layers.Input(shape=(None, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))  # (n_images, height, width, channels)\n",
    "    \n",
    "    # Process each image independently\n",
    "    features = layers.TimeDistributed(feature_extractor)(image_inputs)  # (n_images, FEATURE_DIM)\n",
    "    \n",
    "    # Aggregate features (mean pooling)\n",
    "    aggregated_features = layers.GlobalAveragePooling1D()(features)  # (FEATURE_DIM,)\n",
    "    \n",
    "    # Predict age\n",
    "    outputs = layers.Dense(1)(aggregated_features)  # Regression output for age\n",
    "    \n",
    "    # Build the model\n",
    "    model = models.Model(inputs=image_inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model = build_tree_age_predictor()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # MSE for regression\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI305_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
